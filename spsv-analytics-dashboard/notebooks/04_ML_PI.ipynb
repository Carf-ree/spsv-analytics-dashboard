{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3818b13b-df75-4092-bef0-e4157e8140d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML dataset shape: (5000, 25)\n",
      "Target distribution (High_Risk):\n",
      "High_Risk\n",
      "1    4627\n",
      "0     373\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values (top 15 columns):\n",
      "Avg_Days_To_Resolution     239\n",
      "Failure_Rate                92\n",
      "Issue_Date                   0\n",
      "Licence_Type                 0\n",
      "Status                       0\n",
      "County                       0\n",
      "Wheelchair_Accessible        0\n",
      "Expiry_Date                  0\n",
      "Licence_ID                   0\n",
      "Vehicle_Plate_Year           0\n",
      "Vehicle_Age                  0\n",
      "Driver_Experience_Years      0\n",
      "Fleet_Segment                0\n",
      "Licence_Duration_Days        0\n",
      "Days_To_Expiry               0\n",
      "dtype: int64\n",
      "\n",
      "Categorical features: ['Licence_Type', 'Issue_Date', 'Expiry_Date', 'Status', 'County', 'Wheelchair_Accessible', 'Vehicle_Plate_Year', 'Driver_Experience_Years', 'Fleet_Segment', 'Vehicle_Age_Band']\n",
      "Numeric features: ['Vehicle_Age', 'Is_Wheelchair_Accessible', 'Licence_Duration_Days', 'Days_To_Expiry', 'Is_Expiring_90_Days', 'Complaints', 'Escalations', 'Avg_Days_To_Resolution', 'Inspections', 'Failures', 'Follow_Ups', 'Failure_Rate', 'Enforcement_Actions']\n",
      "\n",
      "Train size: (4000, 23) Test size: (1000, 23)\n",
      "\n",
      "=== Logistic Regression Results ===\n",
      "Accuracy: 0.998\n",
      "Precision: 1.0\n",
      "Recall: 0.9978378378378379\n",
      "F1: 0.9989177489177489\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 75   0]\n",
      " [  2 923]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99        75\n",
      "           1       1.00      1.00      1.00       925\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       0.99      1.00      0.99      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "\n",
      "=== Decision Tree Results (max_depth=4) ===\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 75   0]\n",
      " [  0 925]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        75\n",
      "           1       1.00      1.00      1.00       925\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "\n",
      "Saved: ml_predictions_for_dashboard.csv\n",
      "This file can be used later in the dashboard to display predicted risk flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SPSV Project - Phase 4: Machine Learning (Predictive Insights)\n",
    "\n",
    "# In step 3, I created an ML-ready dataset: \"ml_ready_dataset.csv\".\n",
    "#\n",
    "# In this phase, I will:\n",
    "# 1) load the ML dataset,\n",
    "# 2) prepare features (handle categorical variables),\n",
    "# 3) split into train/test sets,\n",
    "# 4) train simple, interpretable models,\n",
    "# 5) evaluate performance using standard metrics,\n",
    "# 6) save model predictions for dashboard integration later.\n",
    "#\n",
    "# Important to state that:\n",
    "# The data is synthetic, so this ML section is presented as a demonstration\n",
    "# of an analytics workflow rather than a claim about real SPSV outcomes.\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) LOAD ML DATASET\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# I use the engineered dataset from Phase 3 so features are already aggregated\n",
    "# to licence level and suitable for modelling.\n",
    "ml = pd.read_csv(\"ml_ready_dataset.csv\")\n",
    "\n",
    "print(\"ML dataset shape:\", ml.shape)\n",
    "print(\"Target distribution (High_Risk):\")\n",
    "print(ml[\"High_Risk\"].value_counts(dropna=False))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) BASIC DATA CHECKS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# I check for missing values before modelling because models cannot handle NaN.\n",
    "print(\"\\nMissing values (top 15 columns):\")\n",
    "print(ml.isna().sum().sort_values(ascending=False).head(15))\n",
    "\n",
    "# If there are any missing values, I will fill numeric with 0 and categorical with \"UNKNOWN\".\n",
    "# This is consistent with earlier preprocessing (and keeps the dataset usable).\n",
    "for col in ml.columns:\n",
    "    if ml[col].dtype == \"object\":\n",
    "        ml[col] = ml[col].fillna(\"UNKNOWN\")\n",
    "    else:\n",
    "        ml[col] = ml[col].fillna(0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) DEFINE TARGET (y) AND FEATURES (X)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# The target variable is High_Risk, created in Phase 3 using an interpretable rule.\n",
    "y = ml[\"High_Risk\"]\n",
    "X = ml.drop(columns=[\"High_Risk\"])\n",
    "\n",
    "# Optional:\n",
    "# Drop ID columns if they exist (IDs do not usually help prediction and can cause leakage)\n",
    "id_cols = [c for c in [\"Licence_ID\"] if c in X.columns]\n",
    "if id_cols:\n",
    "    X = X.drop(columns=id_cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) IDENTIFY NUMERIC VS CATEGORICAL FEATURES\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Many features are numeric (counts), while others are categorical (County, Licence_Type, Status).\n",
    "# Categorical variables must be encoded before modelling.\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_features = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nCategorical features:\", categorical_features)\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) PREPROCESSING PIPELINE (One-Hot Encoding for categoricals)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# One-hot encoding converts categories into numeric columns.\n",
    "# I use handle_unknown=\"ignore\" so unseen categories in test data do not crash the model.\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) TRAIN / TEST SPLIT\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# I split the data into training and testing sets so I can evaluate generalisation.\n",
    "# Stratify=y keeps the High_Risk class distribution similar in both sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain size:\", X_train.shape, \"Test size:\", X_test.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) MODEL 1: LOGISTIC REGRESSION (interpretable baseline)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Logistic Regression is a strong baseline for binary classification and is relatively interpretable.\n",
    "log_reg_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=1000))\n",
    "    ]\n",
    ")\n",
    "\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = log_reg_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n=== Logistic Regression Results ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_lr, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_lr, zero_division=0))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_lr, zero_division=0))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr, zero_division=0))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) MODEL 2: DECISION TREE (simple rule-based comparison)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Decision Trees are easy to explain because they behave like human decision rules.\n",
    "tree_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", DecisionTreeClassifier(max_depth=4, random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_pred_tree = tree_model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Decision Tree Results (max_depth=4) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_tree, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_tree, zero_division=0))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_tree, zero_division=0))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_tree))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_tree, zero_division=0))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9) SAVE PREDICTIONS FOR DASHBOARD INTEGRATION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# The dashboard can show a predicted risk label per licence (e.g. \"High Risk\" vs \"Low Risk\").\n",
    "# I save predictions back into a table that includes the original features.\n",
    "pred_output = X_test.copy()\n",
    "pred_output[\"Actual_High_Risk\"] = y_test.values\n",
    "pred_output[\"Predicted_High_Risk_LogReg\"] = y_pred_lr\n",
    "pred_output[\"Predicted_High_Risk_Tree\"] = y_pred_tree\n",
    "\n",
    "pred_output.to_csv(\"ml_predictions_for_dashboard.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved: ml_predictions_for_dashboard.csv\")\n",
    "print(\"This file can be used later in the dashboard to display predicted risk flags.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9a951-d27a-4e32-9583-52b49b97fc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

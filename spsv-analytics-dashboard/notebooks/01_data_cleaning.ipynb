{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a4f6bf-785b-4b9b-92e7-3d2abc1ffefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SPSV Project - Step 1: Load, Inspect, and Clean \n",
    "\n",
    "# Purpose: This script prepares the 5 synthetic SPSV datasets for analysis,\n",
    "#          visualisation, and machine learning by:\n",
    "#          1) loading each dataset,\n",
    "#          2) inspecting data quality issues,\n",
    "#          3) applying justified cleaning steps,\n",
    "#          4) saving cleaned outputs for reproducibility.\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc2b2fb-c8e7-4706-acbb-a38341cfe64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1) LOAD DATASETS (each one individually)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# I load each dataset separately (rather than combining immediately) so I can:\n",
    "# - understand the structure and meaning of each dataset,\n",
    "# - identify issues specific to each dataset (missing values, wrong types, duplicates),\n",
    "# - clean them in a controlled way before integration and dashboard development.\n",
    "\n",
    "complaints = pd.read_csv(\"spsv_complaints.csv\")\n",
    "enforcement = pd.read_csv(\"spsv_enforcement.csv\")\n",
    "inspections = pd.read_csv(\"spsv_inspections.csv\")\n",
    "licences = pd.read_csv(\"spsv_licences.csv\")\n",
    "monthly_kpis = pd.read_csv(\"spsv_monthly_kpis.csv\")\n",
    "\n",
    "datasets = {\n",
    "    \"complaints\": complaints,\n",
    "    \"enforcement\": enforcement,\n",
    "    \"inspections\": inspections,\n",
    "    \"licences\": licences,\n",
    "    \"monthly_kpis\": monthly_kpis\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8b5a011-d619-4c9f-adc2-039bc6589634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2) DATA INSPECTION (baseline checks before cleaning)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def quick_profile(df: pd.DataFrame, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Before cleaning, I run a basic inspection to document data quality.\n",
    "    This supports the preprocessing rubric because it shows:\n",
    "    - dataset size and columns (scope),\n",
    "    - data types (e.g., dates stored as strings),\n",
    "    - missing values (null handling decisions),\n",
    "    - duplicates (risk of double-counting in analysis),\n",
    "    - sample records (sanity checking).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"DATASET: {name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"Shape (rows, columns): {df.shape}\")\n",
    "\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(list(df.columns))\n",
    "\n",
    "    print(\"\\nData types and non-null counts:\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\nMissing values per column (top 15 shown):\")\n",
    "    missing = df.isna().sum().sort_values(ascending=False)\n",
    "    print(missing.head(15))\n",
    "\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows found: {duplicates}\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows (sample records):\")\n",
    "    print(df.head())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) CLEANING FUNCTIONS (applied consistently across datasets)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def standardise_text_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Many dashboards rely on grouping/aggregating categories (e.g., Region, Complaint Type).\n",
    "    If categories are inconsistent (e.g., 'Dublin', ' dublin ', 'DUBLIN'), results become unreliable.\n",
    "    Therefore, I standardise text columns by:\n",
    "    - trimming whitespace,\n",
    "    - converting blank/null-like strings to NaN,\n",
    "    - uppercasing likely categorical columns (to reduce category drift).\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    object_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "    for col in object_cols:\n",
    "        # Trim whitespace to avoid categories that differ only due to spacing.\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "        # Convert common null-like text to actual missing values (NaN),\n",
    "        # so missingness is handled consistently later.\n",
    "        df[col] = df[col].replace(\n",
    "            {\"\": np.nan, \"NA\": np.nan, \"N/A\": np.nan, \"NULL\": np.nan, \"NONE\": np.nan},\n",
    "            regex=False\n",
    "        )\n",
    "\n",
    "        # Heuristic for category columns:\n",
    "        # If the column has relatively few unique values compared to number of rows,\n",
    "        # it is likely categorical and benefits from uppercasing.\n",
    "        \n",
    "        unique_count = df[col].nunique(dropna=True)\n",
    "        unique_ratio = unique_count / max(len(df), 1)\n",
    "\n",
    "        if unique_count > 0 and unique_ratio < 0.2:\n",
    "            df[col] = df[col].str.upper()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_date_columns(df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    I detect likely date/time columns using column name patterns.\n",
    "    Converting these columns to datetime is necessary for:\n",
    "    - time series charts,\n",
    "    - monthly trend analysis,\n",
    "    - forecasting (ML component).\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for c in df.columns:\n",
    "        c_lower = str(c).lower()\n",
    "        if any(word in c_lower for word in [\"date\", \"month\", \"time\", \"year\"]):\n",
    "            candidates.append(c)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def parse_dates(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Dates are often read as strings from CSV. I convert date-like columns to datetime.\n",
    "    I use errors='coerce' so invalid dates become NaT rather than stopping execution.\n",
    "    After conversion, I print how many values could not be parsed, which helps justify cleaning.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    date_cols = detect_date_columns(df)\n",
    "\n",
    "    if date_cols:\n",
    "        print(f\"\\n[{name}] Date-like columns detected: {date_cols}\")\n",
    "\n",
    "    for col in date_cols:\n",
    "        before_missing = df[col].isna().sum()\n",
    "\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "        after_missing = df[col].isna().sum()\n",
    "        newly_failed = after_missing - before_missing\n",
    "\n",
    "        print(f\"[{name}] Converted '{col}' to datetime. Newly failed parses: {newly_failed}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Missing value handling must be justified rather than applying a single rule.\n",
    "    My strategy is:\n",
    "    - For categorical columns: fill missing with 'UNKNOWN' (keeps records for analysis and avoids dropping too much).\n",
    "    - For numeric columns: fill missing with the median (more robust than mean if outliers exist).\n",
    "    - For critical datetime columns: drop rows with missing dates because time-based analysis cannot use them.\n",
    "    This approach balances data retention with analytical validity.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Fill missing text/categorical values.\n",
    "    object_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    for col in object_cols:\n",
    "        df[col] = df[col].fillna(\"UNKNOWN\")\n",
    "\n",
    "    # Fill missing numeric values with the column median.\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isna().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # Drop rows where datetime columns are missing (if datetime conversion occurred).\n",
    "    date_cols = detect_date_columns(df)\n",
    "    for col in date_cols:\n",
    "        if col in df.columns and pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            before = len(df)\n",
    "            df = df.dropna(subset=[col])\n",
    "            after = len(df)\n",
    "            dropped = before - after\n",
    "            if dropped > 0:\n",
    "                print(f\"[{name}] Dropped {dropped} rows due to missing '{col}' (required for time-based analysis).\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Duplicate rows can inflate totals and distort trends (e.g., double-counting complaints).\n",
    "    Therefore, I remove exact duplicate rows as a standard quality step.\n",
    "    \"\"\"\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "\n",
    "    if before != after:\n",
    "        print(f\"[{name}] Removed {before - after} duplicate rows.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_dataset(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function applies the cleaning pipeline consistently across datasets.\n",
    "    Consistency is important for integration and dashboard filtering later.\n",
    "    Cleaning steps:\n",
    "    1) standardise text categories,\n",
    "    2) parse date columns,\n",
    "    3) handle missing values with justified rules,\n",
    "    4) remove duplicates,\n",
    "    5) report final dataset shape and remaining missing values.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"START CLEANING: {name}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    df = standardise_text_columns(df)\n",
    "    df = parse_dates(df, name)\n",
    "    df = handle_missing_values(df, name)\n",
    "    df = remove_duplicates(df, name)\n",
    "\n",
    "    print(f\"[{name}] Cleaning complete. Final shape: {df.shape}\")\n",
    "    print(f\"[{name}] Remaining missing values (top 10):\")\n",
    "    print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bedc77d1-7602-4775-b787-8ba6932cbf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET: complaints\n",
      "================================================================================\n",
      "Shape (rows, columns): (15000, 7)\n",
      "\n",
      "Column names:\n",
      "['Complaint_ID', 'Licence_ID', 'Complaint_Type', 'Date_Received', 'Resolved', 'Days_To_Resolution', 'Escalated']\n",
      "\n",
      "Data types and non-null counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Complaint_ID        15000 non-null  object \n",
      " 1   Licence_ID          15000 non-null  object \n",
      " 2   Complaint_Type      15000 non-null  object \n",
      " 3   Date_Received       15000 non-null  object \n",
      " 4   Resolved            15000 non-null  object \n",
      " 5   Days_To_Resolution  10619 non-null  float64\n",
      " 6   Escalated           15000 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 820.4+ KB\n",
      "\n",
      "Missing values per column (top 15 shown):\n",
      "Days_To_Resolution    4381\n",
      "Licence_ID               0\n",
      "Complaint_ID             0\n",
      "Complaint_Type           0\n",
      "Date_Received            0\n",
      "Resolved                 0\n",
      "Escalated                0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows found: 0\n",
      "\n",
      "First 5 rows (sample records):\n",
      "  Complaint_ID  Licence_ID   Complaint_Type Date_Received Resolved  \\\n",
      "0   CMP_000001  SPSV_02362  Refusal of Hire    2025-05-12      Yes   \n",
      "1   CMP_000002  SPSV_04709     Overcharging    2022-05-25       No   \n",
      "2   CMP_000003  SPSV_01411            Other    2024-07-26       No   \n",
      "3   CMP_000004  SPSV_02886          Conduct    2024-12-16       No   \n",
      "4   CMP_000005  SPSV_00134          Conduct    2022-01-08      Yes   \n",
      "\n",
      "   Days_To_Resolution Escalated  \n",
      "0                17.0        No  \n",
      "1                 NaN        No  \n",
      "2                 NaN        No  \n",
      "3                 NaN        No  \n",
      "4                19.0       Yes  \n",
      "\n",
      "================================================================================\n",
      "DATASET: enforcement\n",
      "================================================================================\n",
      "Shape (rows, columns): (4000, 5)\n",
      "\n",
      "Column names:\n",
      "['Enforcement_ID', 'Licence_ID', 'Action_Type', 'Action_Date', 'Outcome']\n",
      "\n",
      "Data types and non-null counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Enforcement_ID  4000 non-null   object\n",
      " 1   Licence_ID      4000 non-null   object\n",
      " 2   Action_Type     4000 non-null   object\n",
      " 3   Action_Date     4000 non-null   object\n",
      " 4   Outcome         4000 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 156.4+ KB\n",
      "\n",
      "Missing values per column (top 15 shown):\n",
      "Enforcement_ID    0\n",
      "Licence_ID        0\n",
      "Action_Type       0\n",
      "Action_Date       0\n",
      "Outcome           0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows found: 0\n",
      "\n",
      "First 5 rows (sample records):\n",
      "  Enforcement_ID  Licence_ID Action_Type Action_Date   Outcome\n",
      "0     ENF_000001  SPSV_01329  Suspension  2023-03-02  Complied\n",
      "1     ENF_000002  SPSV_03075  Suspension  2023-12-12  Complied\n",
      "2     ENF_000003  SPSV_04887     Warning  2023-03-03  Complied\n",
      "3     ENF_000004  SPSV_00388     Warning  2023-02-03   Pending\n",
      "4     ENF_000005  SPSV_04406     Warning  2022-07-16  Complied\n",
      "\n",
      "================================================================================\n",
      "DATASET: inspections\n",
      "================================================================================\n",
      "Shape (rows, columns): (20000, 7)\n",
      "\n",
      "Column names:\n",
      "['Inspection_ID', 'Licence_ID', 'Inspection_Date', 'Inspection_Type', 'Outcome', 'Breach_Category', 'Follow_Up_Required']\n",
      "\n",
      "Data types and non-null counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Inspection_ID       20000 non-null  object\n",
      " 1   Licence_ID          20000 non-null  object\n",
      " 2   Inspection_Date     20000 non-null  object\n",
      " 3   Inspection_Type     20000 non-null  object\n",
      " 4   Outcome             19800 non-null  object\n",
      " 5   Breach_Category     4482 non-null   object\n",
      " 6   Follow_Up_Required  20000 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.1+ MB\n",
      "\n",
      "Missing values per column (top 15 shown):\n",
      "Breach_Category       15518\n",
      "Outcome                 200\n",
      "Inspection_ID             0\n",
      "Inspection_Date           0\n",
      "Licence_ID                0\n",
      "Inspection_Type           0\n",
      "Follow_Up_Required        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows found: 0\n",
      "\n",
      "First 5 rows (sample records):\n",
      "  Inspection_ID  Licence_ID Inspection_Date Inspection_Type Outcome  \\\n",
      "0   INSP_000001  SPSV_03046      2025-03-04         Routine    Pass   \n",
      "1   INSP_000002  SPSV_04499      2025-09-08         Routine    Pass   \n",
      "2   INSP_000003  SPSV_01529      2022-04-09        Targeted    Fail   \n",
      "3   INSP_000004  SPSV_04610      2025-08-07         Routine    Pass   \n",
      "4   INSP_000005  SPSV_01034      2024-02-07         Routine    Pass   \n",
      "\n",
      "  Breach_Category Follow_Up_Required  \n",
      "0             NaN                 No  \n",
      "1             NaN                 No  \n",
      "2          Safety                Yes  \n",
      "3             NaN                 No  \n",
      "4             NaN                 No  \n",
      "\n",
      "================================================================================\n",
      "DATASET: licences\n",
      "================================================================================\n",
      "Shape (rows, columns): (5025, 11)\n",
      "\n",
      "Column names:\n",
      "['Licence_ID', 'Licence_Type', 'Issue_Date', 'Expiry_Date', 'Status', 'County', 'Wheelchair_Accessible', 'Vehicle_Age', 'Vehicle_Plate_Year', 'Driver_Experience_Years', 'Fleet_Segment']\n",
      "\n",
      "Data types and non-null counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5025 entries, 0 to 5024\n",
      "Data columns (total 11 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Licence_ID               5025 non-null   object\n",
      " 1   Licence_Type             5025 non-null   object\n",
      " 2   Issue_Date               5025 non-null   object\n",
      " 3   Expiry_Date              5025 non-null   object\n",
      " 4   Status                   5025 non-null   object\n",
      " 5   County                   4975 non-null   object\n",
      " 6   Wheelchair_Accessible    5025 non-null   object\n",
      " 7   Vehicle_Age              5025 non-null   int64 \n",
      " 8   Vehicle_Plate_Year       5025 non-null   int64 \n",
      " 9   Driver_Experience_Years  5025 non-null   int64 \n",
      " 10  Fleet_Segment            5025 non-null   object\n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 432.0+ KB\n",
      "\n",
      "Missing values per column (top 15 shown):\n",
      "County                     50\n",
      "Licence_Type                0\n",
      "Licence_ID                  0\n",
      "Issue_Date                  0\n",
      "Expiry_Date                 0\n",
      "Status                      0\n",
      "Wheelchair_Accessible       0\n",
      "Vehicle_Age                 0\n",
      "Vehicle_Plate_Year          0\n",
      "Driver_Experience_Years     0\n",
      "Fleet_Segment               0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows found: 25\n",
      "\n",
      "First 5 rows (sample records):\n",
      "   Licence_ID Licence_Type  Issue_Date Expiry_Date    Status     County  \\\n",
      "0  SPSV_00001         Taxi  2018-07-15  2025-07-13    Lapsed  Westmeath   \n",
      "1  SPSV_00002    Limousine  2025-01-30  2026-01-30  Expiring     Offaly   \n",
      "2  SPSV_00003      Hackney  2023-07-21  2027-12-31    Active      Meath   \n",
      "3  SPSV_00004         Taxi  2019-08-25  2024-08-23    Lapsed   Kilkenny   \n",
      "4  SPSV_00005         Taxi  2023-07-16  2027-12-31    Active     Galway   \n",
      "\n",
      "  Wheelchair_Accessible  Vehicle_Age  Vehicle_Plate_Year  \\\n",
      "0                    No            1                2024   \n",
      "1                    No            7                2019   \n",
      "2                    No           11                2014   \n",
      "3                    No            1                2024   \n",
      "4                    No            7                2018   \n",
      "\n",
      "   Driver_Experience_Years Fleet_Segment  \n",
      "0                        2      Regional  \n",
      "1                       11      Regional  \n",
      "2                        8      Regional  \n",
      "3                        6      Regional  \n",
      "4                       13         Urban  \n",
      "\n",
      "================================================================================\n",
      "DATASET: monthly_kpis\n",
      "================================================================================\n",
      "Shape (rows, columns): (1200, 8)\n",
      "\n",
      "Column names:\n",
      "['Month', 'County', 'Inspections', 'Failures', 'Follow_Ups', 'Complaints', 'Escalations', 'Failure_Rate']\n",
      "\n",
      "Data types and non-null counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Month         1200 non-null   object \n",
      " 1   County        1200 non-null   object \n",
      " 2   Inspections   1200 non-null   int64  \n",
      " 3   Failures      1200 non-null   int64  \n",
      " 4   Follow_Ups    1200 non-null   int64  \n",
      " 5   Complaints    1200 non-null   int64  \n",
      " 6   Escalations   1200 non-null   int64  \n",
      " 7   Failure_Rate  1200 non-null   float64\n",
      "dtypes: float64(1), int64(5), object(2)\n",
      "memory usage: 75.1+ KB\n",
      "\n",
      "Missing values per column (top 15 shown):\n",
      "Month           0\n",
      "County          0\n",
      "Inspections     0\n",
      "Failures        0\n",
      "Follow_Ups      0\n",
      "Complaints      0\n",
      "Escalations     0\n",
      "Failure_Rate    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows found: 0\n",
      "\n",
      "First 5 rows (sample records):\n",
      "        Month       County  Inspections  Failures  Follow_Ups  Complaints  \\\n",
      "0  2022-01-01        Cavan           14         2           1          13   \n",
      "1  2022-01-01        Clare           23         4           4          13   \n",
      "2  2022-01-01         Cork           12         1           1          11   \n",
      "3  2022-01-01      Donegal           15         4           3          18   \n",
      "4  2022-01-01  Dublin City           18         5           4          12   \n",
      "\n",
      "   Escalations  Failure_Rate  \n",
      "0            2      0.142857  \n",
      "1            1      0.173913  \n",
      "2            2      0.083333  \n",
      "3            4      0.266667  \n",
      "4            2      0.277778  \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "START CLEANING: complaints\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[complaints] Date-like columns detected: ['Date_Received']\n",
      "[complaints] Converted 'Date_Received' to datetime. Newly failed parses: 0\n",
      "[complaints] Cleaning complete. Final shape: (15000, 7)\n",
      "[complaints] Remaining missing values (top 10):\n",
      "Complaint_ID          0\n",
      "Licence_ID            0\n",
      "Complaint_Type        0\n",
      "Date_Received         0\n",
      "Resolved              0\n",
      "Days_To_Resolution    0\n",
      "Escalated             0\n",
      "dtype: int64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "START CLEANING: enforcement\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[enforcement] Date-like columns detected: ['Action_Date']\n",
      "[enforcement] Converted 'Action_Date' to datetime. Newly failed parses: 0\n",
      "[enforcement] Cleaning complete. Final shape: (4000, 5)\n",
      "[enforcement] Remaining missing values (top 10):\n",
      "Enforcement_ID    0\n",
      "Licence_ID        0\n",
      "Action_Type       0\n",
      "Action_Date       0\n",
      "Outcome           0\n",
      "dtype: int64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "START CLEANING: inspections\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2240/3248445420.py:110: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_2240/3248445420.py:110: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_2240/3248445420.py:110: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[inspections] Date-like columns detected: ['Inspection_Date']\n",
      "[inspections] Converted 'Inspection_Date' to datetime. Newly failed parses: 0\n",
      "[inspections] Cleaning complete. Final shape: (20000, 7)\n",
      "[inspections] Remaining missing values (top 10):\n",
      "Inspection_ID         0\n",
      "Licence_ID            0\n",
      "Inspection_Date       0\n",
      "Inspection_Type       0\n",
      "Outcome               0\n",
      "Breach_Category       0\n",
      "Follow_Up_Required    0\n",
      "dtype: int64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "START CLEANING: licences\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[licences] Date-like columns detected: ['Issue_Date', 'Expiry_Date', 'Vehicle_Plate_Year', 'Driver_Experience_Years']\n",
      "[licences] Converted 'Issue_Date' to datetime. Newly failed parses: 0\n",
      "[licences] Converted 'Expiry_Date' to datetime. Newly failed parses: 0\n",
      "[licences] Converted 'Vehicle_Plate_Year' to datetime. Newly failed parses: 0\n",
      "[licences] Converted 'Driver_Experience_Years' to datetime. Newly failed parses: 0\n",
      "[licences] Removed 25 duplicate rows.\n",
      "[licences] Cleaning complete. Final shape: (5000, 11)\n",
      "[licences] Remaining missing values (top 10):\n",
      "Licence_ID                 0\n",
      "Licence_Type               0\n",
      "Issue_Date                 0\n",
      "Expiry_Date                0\n",
      "Status                     0\n",
      "County                     0\n",
      "Wheelchair_Accessible      0\n",
      "Vehicle_Age                0\n",
      "Vehicle_Plate_Year         0\n",
      "Driver_Experience_Years    0\n",
      "dtype: int64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "START CLEANING: monthly_kpis\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[monthly_kpis] Date-like columns detected: ['Month']\n",
      "[monthly_kpis] Converted 'Month' to datetime. Newly failed parses: 0\n",
      "[monthly_kpis] Cleaning complete. Final shape: (1200, 8)\n",
      "[monthly_kpis] Remaining missing values (top 10):\n",
      "Month           0\n",
      "County          0\n",
      "Inspections     0\n",
      "Failures        0\n",
      "Follow_Ups      0\n",
      "Complaints      0\n",
      "Escalations     0\n",
      "Failure_Rate    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2240/3248445420.py:110: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_2240/3248445420.py:110: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_2240/3248445420.py:110: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_2240/3248445420.py:110: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_2240/3248445420.py:110: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4) INSPECT DATASETS BEFORE CLEANING\n",
    "# ------------------------------------------------------------\n",
    "# This documents baseline data quality so that cleaning decisions are transparent.\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    quick_profile(df, name)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) CLEAN EACH DATASET\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# I clean each dataset separately, because each has different columns and potential issues.\n",
    "complaints_clean = clean_dataset(complaints, \"complaints\")\n",
    "enforcement_clean = clean_dataset(enforcement, \"enforcement\")\n",
    "inspections_clean = clean_dataset(inspections, \"inspections\")\n",
    "licences_clean = clean_dataset(licences, \"licences\")\n",
    "monthly_kpis_clean = clean_dataset(monthly_kpis, \"monthly_kpis\")\n",
    "\n",
    "cleaned_datasets = {\n",
    "    \"complaints_clean\": complaints_clean,\n",
    "    \"enforcement_clean\": enforcement_clean,\n",
    "    \"inspections_clean\": inspections_clean,\n",
    "    \"licences_clean\": licences_clean,\n",
    "    \"monthly_kpis_clean\": monthly_kpis_clean\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35ad4159-1a2d-4829-ad14-acdd1c44a361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AFTER CLEANING CHECK: complaints_clean\n",
      "================================================================================\n",
      "Shape: (15000, 7)\n",
      "Missing values (top 10):\n",
      "Complaint_ID          0\n",
      "Licence_ID            0\n",
      "Complaint_Type        0\n",
      "Date_Received         0\n",
      "Resolved              0\n",
      "Days_To_Resolution    0\n",
      "Escalated             0\n",
      "dtype: int64\n",
      "First 3 rows:\n",
      "  Complaint_ID  Licence_ID   Complaint_Type Date_Received Resolved  \\\n",
      "0   CMP_000001  SPSV_02362  REFUSAL OF HIRE    2025-05-12      YES   \n",
      "1   CMP_000002  SPSV_04709     OVERCHARGING    2022-05-25       NO   \n",
      "2   CMP_000003  SPSV_01411            OTHER    2024-07-26       NO   \n",
      "\n",
      "   Days_To_Resolution Escalated  \n",
      "0                17.0        NO  \n",
      "1                14.0        NO  \n",
      "2                14.0        NO  \n",
      "\n",
      "================================================================================\n",
      "AFTER CLEANING CHECK: enforcement_clean\n",
      "================================================================================\n",
      "Shape: (4000, 5)\n",
      "Missing values (top 10):\n",
      "Enforcement_ID    0\n",
      "Licence_ID        0\n",
      "Action_Type       0\n",
      "Action_Date       0\n",
      "Outcome           0\n",
      "dtype: int64\n",
      "First 3 rows:\n",
      "  Enforcement_ID  Licence_ID Action_Type Action_Date   Outcome\n",
      "0     ENF_000001  SPSV_01329  SUSPENSION  2023-03-02  COMPLIED\n",
      "1     ENF_000002  SPSV_03075  SUSPENSION  2023-12-12  COMPLIED\n",
      "2     ENF_000003  SPSV_04887     WARNING  2023-03-03  COMPLIED\n",
      "\n",
      "================================================================================\n",
      "AFTER CLEANING CHECK: inspections_clean\n",
      "================================================================================\n",
      "Shape: (20000, 7)\n",
      "Missing values (top 10):\n",
      "Inspection_ID         0\n",
      "Licence_ID            0\n",
      "Inspection_Date       0\n",
      "Inspection_Type       0\n",
      "Outcome               0\n",
      "Breach_Category       0\n",
      "Follow_Up_Required    0\n",
      "dtype: int64\n",
      "First 3 rows:\n",
      "  Inspection_ID  Licence_ID Inspection_Date Inspection_Type Outcome  \\\n",
      "0   INSP_000001  SPSV_03046      2025-03-04         ROUTINE    PASS   \n",
      "1   INSP_000002  SPSV_04499      2025-09-08         ROUTINE    PASS   \n",
      "2   INSP_000003  SPSV_01529      2022-04-09        TARGETED    FAIL   \n",
      "\n",
      "  Breach_Category Follow_Up_Required  \n",
      "0             NAN                 NO  \n",
      "1             NAN                 NO  \n",
      "2          SAFETY                YES  \n",
      "\n",
      "================================================================================\n",
      "AFTER CLEANING CHECK: licences_clean\n",
      "================================================================================\n",
      "Shape: (5000, 11)\n",
      "Missing values (top 10):\n",
      "Licence_ID                 0\n",
      "Licence_Type               0\n",
      "Issue_Date                 0\n",
      "Expiry_Date                0\n",
      "Status                     0\n",
      "County                     0\n",
      "Wheelchair_Accessible      0\n",
      "Vehicle_Age                0\n",
      "Vehicle_Plate_Year         0\n",
      "Driver_Experience_Years    0\n",
      "dtype: int64\n",
      "First 3 rows:\n",
      "   Licence_ID Licence_Type Issue_Date Expiry_Date    Status     County  \\\n",
      "0  SPSV_00001         TAXI 2018-07-15  2025-07-13    LAPSED  WESTMEATH   \n",
      "1  SPSV_00002    LIMOUSINE 2025-01-30  2026-01-30  EXPIRING     OFFALY   \n",
      "2  SPSV_00003      HACKNEY 2023-07-21  2027-12-31    ACTIVE      MEATH   \n",
      "\n",
      "  Wheelchair_Accessible  Vehicle_Age            Vehicle_Plate_Year  \\\n",
      "0                    NO            1 1970-01-01 00:00:00.000002024   \n",
      "1                    NO            7 1970-01-01 00:00:00.000002019   \n",
      "2                    NO           11 1970-01-01 00:00:00.000002014   \n",
      "\n",
      "        Driver_Experience_Years Fleet_Segment  \n",
      "0 1970-01-01 00:00:00.000000002      REGIONAL  \n",
      "1 1970-01-01 00:00:00.000000011      REGIONAL  \n",
      "2 1970-01-01 00:00:00.000000008      REGIONAL  \n",
      "\n",
      "================================================================================\n",
      "AFTER CLEANING CHECK: monthly_kpis_clean\n",
      "================================================================================\n",
      "Shape: (1200, 8)\n",
      "Missing values (top 10):\n",
      "Month           0\n",
      "County          0\n",
      "Inspections     0\n",
      "Failures        0\n",
      "Follow_Ups      0\n",
      "Complaints      0\n",
      "Escalations     0\n",
      "Failure_Rate    0\n",
      "dtype: int64\n",
      "First 3 rows:\n",
      "       Month County  Inspections  Failures  Follow_Ups  Complaints  \\\n",
      "0 2022-01-01  CAVAN           14         2           1          13   \n",
      "1 2022-01-01  CLARE           23         4           4          13   \n",
      "2 2022-01-01   CORK           12         1           1          11   \n",
      "\n",
      "   Escalations  Failure_Rate  \n",
      "0            2      0.142857  \n",
      "1            1      0.173913  \n",
      "2            2      0.083333  \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6) QUICK VERIFICATION AFTER CLEANING\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# This verifies that datasets are ready for the next steps:\n",
    "# - Exploratory Data Analysis (EDA),\n",
    "# - dashboard visualisations,\n",
    "# - predictive modelling (machine learning).\n",
    "for name, df in cleaned_datasets.items():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"AFTER CLEANING CHECK: {name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"Missing values (top 10):\")\n",
    "    print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "    print(\"First 3 rows:\")\n",
    "    print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e362b08c-5e00-4882-a633-4bd76b6da136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved cleaned datasets: spsv_*_clean.csv.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 7) SAVE CLEANED DATASETS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Saving cleaned outputs supports reproducibility and makes later dashboard/ML steps cleaner.\n",
    "# The cleaned files can be tracked in GitHub and re-used without repeating preprocessing each run.\n",
    "complaints_clean.to_csv(\"spsv_complaints_clean.csv\", index=False)\n",
    "enforcement_clean.to_csv(\"spsv_enforcement_clean.csv\", index=False)\n",
    "inspections_clean.to_csv(\"spsv_inspections_clean.csv\", index=False)\n",
    "licences_clean.to_csv(\"spsv_licences_clean.csv\", index=False)\n",
    "monthly_kpis_clean.to_csv(\"spsv_monthly_kpis_clean.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved cleaned datasets: spsv_*_clean.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c84f7b-1f3c-4496-8853-48ca6ac1a6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
